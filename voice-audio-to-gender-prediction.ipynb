{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583ff41d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:16.546472Z",
     "iopub.status.busy": "2024-06-27T16:55:16.546042Z",
     "iopub.status.idle": "2024-06-27T16:55:19.627291Z",
     "shell.execute_reply": "2024-06-27T16:55:19.625868Z"
    },
    "papermill": {
     "duration": 3.093573,
     "end_time": "2024-06-27T16:55:19.629959",
     "exception": false,
     "start_time": "2024-06-27T16:55:16.536386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa as lib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a854761",
   "metadata": {
    "papermill": {
     "duration": 0.005801,
     "end_time": "2024-06-27T16:55:19.642337",
     "exception": false,
     "start_time": "2024-06-27T16:55:19.636536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This is project regarding Voice sample to Gender\n",
    "\n",
    "* **Dataset** :  Common voice dataset is used here in this project\n",
    "\n",
    "### Libraries\n",
    "* Numpy (for mathematical calculations)\n",
    "* Pandas (To read dataset)\n",
    "* Matplotlib and Seaborn (visualization)\n",
    "* Librosa (Audio analysis)\n",
    "* sklearn (model training and analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c89e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:19.656648Z",
     "iopub.status.busy": "2024-06-27T16:55:19.655923Z",
     "iopub.status.idle": "2024-06-27T16:55:20.201913Z",
     "shell.execute_reply": "2024-06-27T16:55:20.200606Z"
    },
    "papermill": {
     "duration": 0.55567,
     "end_time": "2024-06-27T16:55:20.204310",
     "exception": false,
     "start_time": "2024-06-27T16:55:19.648640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-other-train/sample-000000.mp3</td>\n",
       "      <td>he had to spit some tobacco out of his mouth</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>seventies</td>\n",
       "      <td>male</td>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-other-train/sample-000001.mp3</td>\n",
       "      <td>it took her a while to get used to it</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>scotland</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-other-train/sample-000002.mp3</td>\n",
       "      <td>you will need some rubber boots</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-other-train/sample-000003.mp3</td>\n",
       "      <td>you can speak a label to click on an element</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fourties</td>\n",
       "      <td>male</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-other-train/sample-000004.mp3</td>\n",
       "      <td>the priest collapsed backwards</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0  cv-other-train/sample-000000.mp3   \n",
       "1  cv-other-train/sample-000001.mp3   \n",
       "2  cv-other-train/sample-000002.mp3   \n",
       "3  cv-other-train/sample-000003.mp3   \n",
       "4  cv-other-train/sample-000004.mp3   \n",
       "\n",
       "                                           text  up_votes  down_votes  \\\n",
       "0  he had to spit some tobacco out of his mouth         0           0   \n",
       "1         it took her a while to get used to it         1           1   \n",
       "2               you will need some rubber boots         0           0   \n",
       "3  you can speak a label to click on an element         0           0   \n",
       "4                the priest collapsed backwards         0           0   \n",
       "\n",
       "         age gender    accent  duration  \n",
       "0  seventies   male   england       NaN  \n",
       "1   twenties   male  scotland       NaN  \n",
       "2        NaN    NaN       NaN       NaN  \n",
       "3   fourties   male        us       NaN  \n",
       "4        NaN    NaN       NaN       NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/common-voice/cv-other-train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd30e6a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:20.218248Z",
     "iopub.status.busy": "2024-06-27T16:55:20.217869Z",
     "iopub.status.idle": "2024-06-27T16:55:20.263139Z",
     "shell.execute_reply": "2024-06-27T16:55:20.261815Z"
    },
    "papermill": {
     "duration": 0.055077,
     "end_time": "2024-06-27T16:55:20.265647",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.210570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename           0\n",
       "text               2\n",
       "up_votes           0\n",
       "down_votes         0\n",
       "age            81433\n",
       "gender         81490\n",
       "accent         90559\n",
       "duration      145135\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc52e2f",
   "metadata": {
    "papermill": {
     "duration": 0.007179,
     "end_time": "2024-06-27T16:55:20.279146",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.271967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data cleaning\n",
    "\n",
    "* Here I converted the genders (Here **Male** and **Female**) into 0 and 1 \n",
    "* After that dropped all the rows which have NA as lable in gender\n",
    "* Then dropped all other columns which will not be use in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eed2ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:20.293733Z",
     "iopub.status.busy": "2024-06-27T16:55:20.293361Z",
     "iopub.status.idle": "2024-06-27T16:55:20.336897Z",
     "shell.execute_reply": "2024-06-27T16:55:20.335682Z"
    },
    "papermill": {
     "duration": 0.053328,
     "end_time": "2024-06-27T16:55:20.339204",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.285876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-other-train/sample-000000.mp3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-other-train/sample-000001.mp3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-other-train/sample-000003.mp3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cv-other-train/sample-000005.mp3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cv-other-train/sample-000007.mp3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  gender\n",
       "0  cv-other-train/sample-000000.mp3     1.0\n",
       "1  cv-other-train/sample-000001.mp3     1.0\n",
       "3  cv-other-train/sample-000003.mp3     1.0\n",
       "5  cv-other-train/sample-000005.mp3     1.0\n",
       "7  cv-other-train/sample-000007.mp3     0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = {\n",
    "    'male':1,\n",
    "    'female':0\n",
    "}\n",
    "data['gender'] = data['gender'].map(gen)\n",
    "data=data.dropna(subset=['gender'])\n",
    "data=data.drop(columns=['text','up_votes','down_votes','age','accent','duration'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9609d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:20.353687Z",
     "iopub.status.busy": "2024-06-27T16:55:20.353286Z",
     "iopub.status.idle": "2024-06-27T16:55:20.367770Z",
     "shell.execute_reply": "2024-06-27T16:55:20.366533Z"
    },
    "papermill": {
     "duration": 0.024811,
     "end_time": "2024-06-27T16:55:20.370442",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.345631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename    0\n",
       "gender      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85046d6",
   "metadata": {
    "papermill": {
     "duration": 0.006726,
     "end_time": "2024-06-27T16:55:20.383840",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.377114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Audio Feature Extraction Fuction\n",
    "\n",
    "### Memoize Function \n",
    "* **Cache Dictionary**: Stores results of function calls.\n",
    "* **@wraps**: Preserves the original function’s metadata.\n",
    "* **Wrapper Function**: Checks if the result is in the cache. If not, it computes and stores it.\n",
    "\n",
    "### Audio feature extraction:\n",
    "This code has few components\n",
    "1. At first I remove the silent portions from the audio (Here silence means certain sound portion in a audio portion)\n",
    "2. Ensuring the audio size after removing some portions of the audio\n",
    "3. After that I have adjust some parameters dynamically (Here fft stands for Fast Fourier Transform , mels for Melody)\n",
    "4. Next I get the MFCC(Mel-Frequency Cepstral Coefficient) from the audio\n",
    "5. After that I extracted the other audio features like Chroma and Zero crossing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3e449c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:20.399492Z",
     "iopub.status.busy": "2024-06-27T16:55:20.399085Z",
     "iopub.status.idle": "2024-06-27T16:55:20.413467Z",
     "shell.execute_reply": "2024-06-27T16:55:20.412052Z"
    },
    "papermill": {
     "duration": 0.024944,
     "end_time": "2024-06-27T16:55:20.415784",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.390840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature extraction funtion \n",
    "# MFCC data will be extracted from the audio\n",
    "\n",
    "from functools import wraps\n",
    "import os\n",
    "\n",
    "def memoize(f):\n",
    "    cache = {}\n",
    "    \n",
    "    @wraps(f)\n",
    "    def wrapper(file_path):\n",
    "        if file_path not in cache:\n",
    "            cache[file_path] = f(file_path)\n",
    "        return cache[file_path]\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@memoize\n",
    "def extract_audio_feature(file_path):\n",
    "    path = \"/kaggle/input/common-voice/cv-other-train/\"\n",
    "    dt, sr = lib.load(path + file_path, sr=None) \n",
    "\n",
    "    # Remove silent portions\n",
    "    non_silent_intervals = lib.effects.split(dt, top_db=20) \n",
    "    non_silent_audio = np.concatenate([dt[start:end] for start, end in non_silent_intervals])\n",
    "\n",
    "    # Minimum lenght\n",
    "    min_length = 2048\n",
    "    if len(non_silent_audio) < min_length:\n",
    "        non_silent_audio = np.pad(non_silent_audio, (0, min_length - len(non_silent_audio)), mode='constant')\n",
    "\n",
    "    # Adjust parameters dynamically\n",
    "    n_fft = min(len(non_silent_audio), 512) \n",
    "    n_mels = min(40, len(non_silent_audio) // 20)  # Adjust n_mels based on audio length\n",
    "    fmax = sr // 2 if sr // 2 < 8000 else 8000  # Adjust fmax based on sampling rate\n",
    "\n",
    "    # Compute MFCC features\n",
    "    mfcc_data = lib.feature.mfcc(y=non_silent_audio, sr=sr, n_mfcc=40, n_fft=n_fft, n_mels=n_mels, fmax=fmax)\n",
    "    \n",
    "    # Compute additional statistical features from MFCC\n",
    "    mfcc_mean = np.mean(mfcc_data.T, axis=0)\n",
    "    mfcc_std = np.std(mfcc_data.T, axis=0)\n",
    "    \n",
    "    # Compute other basic features with adjusted n_fft\n",
    "    chroma_stft = lib.feature.chroma_stft(y=non_silent_audio, sr=sr, n_fft=n_fft)\n",
    "    chroma_mean = np.mean(chroma_stft.T, axis=0)\n",
    "    chroma_std = np.std(chroma_stft.T, axis=0)\n",
    "    \n",
    "    zero_crossing_rate = lib.feature.zero_crossing_rate(y=non_silent_audio)\n",
    "    zcr_mean = np.mean(zero_crossing_rate.T, axis=0)\n",
    "    zcr_std = np.std(zero_crossing_rate.T, axis=0)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = np.concatenate((mfcc_mean, mfcc_std, chroma_mean, chroma_std, zcr_mean, zcr_std))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    return [extract_audio_feature(file_path) for file_path in chunk]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3592622",
   "metadata": {
    "papermill": {
     "duration": 0.006142,
     "end_time": "2024-06-27T16:55:20.428454",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.422312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature and Label creation\n",
    "\n",
    "X is the dataframe of all the audio paths\n",
    "y is the dataframe for the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f6be2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:20.443076Z",
     "iopub.status.busy": "2024-06-27T16:55:20.442623Z",
     "iopub.status.idle": "2024-06-27T16:55:20.465096Z",
     "shell.execute_reply": "2024-06-27T16:55:20.463669Z"
    },
    "papermill": {
     "duration": 0.032756,
     "end_time": "2024-06-27T16:55:20.467622",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.434866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28463\n",
      "<class 'list'>\n",
      "28463\n"
     ]
    }
   ],
   "source": [
    "X = data['filename']\n",
    "y = data['gender']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.55)\n",
    "\n",
    "X_train = X_train.tolist()\n",
    "y_train = y_train.tolist()\n",
    "\n",
    "print(len(X_train))\n",
    "print(type(X_train))\n",
    "print(len(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6267c",
   "metadata": {
    "papermill": {
     "duration": 0.0063,
     "end_time": "2024-06-27T16:55:20.480624",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.474324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature extraction\n",
    "\n",
    "Here we have made a chunks of data(divided data in several parts)so that we can use all the CPUs to enhance the speed of data extraction\n",
    "\n",
    "Pool is used for create the multiprocessing envioronment and uses all the cpu(Here 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d6e2f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:55:20.496199Z",
     "iopub.status.busy": "2024-06-27T16:55:20.495779Z",
     "iopub.status.idle": "2024-06-27T17:16:15.046945Z",
     "shell.execute_reply": "2024-06-27T17:16:15.045366Z"
    },
    "papermill": {
     "duration": 1254.562762,
     "end_time": "2024-06-27T17:16:15.049988",
     "exception": false,
     "start_time": "2024-06-27T16:55:20.487226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/opt/conda/lib/python3.10/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/opt/conda/lib/python3.10/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "/opt/conda/lib/python3.10/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "cpu_count = os.cpu_count()\n",
    "chunk_len = len(X_train) // cpu_count\n",
    "chunks = [X_train[i:i + chunk_len] for i in range(0, len(X_train), chunk_len)]\n",
    "\n",
    "with Pool(processes=cpu_count) as pool:\n",
    "    results = pool.map(process_chunk, chunks)\n",
    "\n",
    "# Flatten the list of results if necessary\n",
    "all_res = [feature for sublist in results for feature in sublist]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d7e7f",
   "metadata": {
    "papermill": {
     "duration": 0.00646,
     "end_time": "2024-06-27T17:16:15.064261",
     "exception": false,
     "start_time": "2024-06-27T17:16:15.057801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Shaping and prepare all the data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "102a6790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T17:16:15.079869Z",
     "iopub.status.busy": "2024-06-27T17:16:15.079427Z",
     "iopub.status.idle": "2024-06-27T17:16:15.116113Z",
     "shell.execute_reply": "2024-06-27T17:16:15.114749Z"
    },
    "papermill": {
     "duration": 0.047701,
     "end_time": "2024-06-27T17:16:15.118717",
     "exception": false,
     "start_time": "2024-06-27T17:16:15.071016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "28463\n",
      "28463\n"
     ]
    }
   ],
   "source": [
    "all_res = np.array(all_res)\n",
    "gens = np.array([y_train],dtype=np.float64)\n",
    "gens_ = gens.reshape(-1,1)\n",
    "print(type(all_res))\n",
    "print(gens_)\n",
    "print(len(gens_))\n",
    "print(len(all_res))"
   ]
  },
  {
   "attachments": {
    "358609bf-f7b8-4dc6-a6d3-f9e250b38885.gif": {
     "image/gif": "R0lGODlhDQEmALMAAP///wAAALq6ujIyMqqqqpiYmNzc3BAQEMzMzFRUVGZmZkRERHZ2du7u7oiIiCIiIiH5BAEAAAAALAAAAAANASYAAAT+EMhJq73UCcxtU10ojmRpnmippWz6tafjwBOy0Hiu76TM/7VbSjAoEkKExK6gBDqf0E4yymOiCoGN4YDgGB4NXmJGLZt13/ClwVgoEonumTM2GQLNQQCEGRx5DQEGc4SFJH4YAgd/AF8MhhWBgyQLAUdYAY8XBQNObZCgoQCcGAiWFVtkoZ8jdwFhCAcDahaLToEborpnthcPnRYMgqK4Iw4BDyMFB1Bwu89RyxgEAQUXd3yhziKVTSED2UACr9DlP+AYlXIWAQG647Qc7dYhd7lP1eb6OPYY7fEUHmTRla+Dq3UcjlFZAGyfQxQKL5hyly6AKlAMQ2AakTFKxIf+ICk1rECNGYYEeHR9rHCgnUuXIyvsEfFhwIMFkwA4WHATYYVxPqExsKlJws4iA+hJEJCA4YCiHBAkQHoPQ82bOXf2ZBeOAhaTFxQEEALjKs4JWhcEnQB0TYMGAhW8BUjhzsU1D+gpODCIARkHB+hKsLuPwaNxShDknSDsUYMFDNSYillBLJkGAwTsDQpXL18Afo0GrmsRw7FkGMRSPtFZwt6+f0dfKy2vYIdxVT08uBcogebBLRlxNYFAgPHjyJMrz90BWCBLsyg8x5xbj/AJmAdiF6h9O288vxsFp4AbgzDUYZGV3Y0dPJ8tp/x1pTAxJwcsaycouNsyOgCx8V3+cABZNL1k4IEIBpBfBQWQMZFsFLTDXgWVQDWBHncBUAlFFOzHUgD+ASgcfqmpVyJ6KXhIQX9qiMjBgPe1M4Iw9qGCogQteUPAAd74cuMuZ43STlCuWIgjbR2aWIEeBDbyY44T7NgjaMOk96N+ILbwRS0pSSBlBw9cKQFKqwVDjnlKDdbOdd+IucoEKDUpASZ0PZebKxkC0M5dDKTZyJohCCMYlW6KJWcJfaICaAk2cRDXCGIN2ogFx4B1iKX6tJQnmRdQwyGc/1gwjoKoUBoApukNWikHhmppKqrfwCrBoiFEaoIe88X60ERraXpBnOxkaYFYsi45U61nWrDRSV3+6oDrCQPIWt+MyYpw0AlhPrSqBdcG6+eoRgIg0JSzkRpotfTJ2AeS/AyJrZjHfNoBjSqc6kEI2ZLwXIL8vrTgBd1soi5JA0+ACXPPsfnqvRXQ60+VXDLXwrbS4SsmsMqYO8KjFbiJ46EcCEDAyCSXbPLJEncwD8DNUhCwl4PES5enaghQ4wQcB6SsxhZUknLBOOSMcwgw+pIJCeMoDIBiZE3kJwG5Rhj1O+3crKdtE7hyD2qe/iqshoMwXcPKFEAtaoA7h2sKuSWILYHTJE09a6526stuQFWPqUeacEn6p59CKZkuxAbbu9R7QC/dDh+YScDdJFNh3bcHd7N0pVj+Nxvmt+N5AxA532AYhDVboZLAIwbjDqaAKeEsADjB/4byrJmwHuONf57zbABDtDlAT+qNrH6sBK5jcHrIo5uSYbxl4t2EAcK3/nrZPDMmuAhTmRf6Y2GgNMhj02cgLzQJX6BHuHc04Zt0shisxDggGNAQA9sv0P0w4NPRvJe9OBkugIS7AP3CwL3cfa94IViJy4aXsUHRbwAJoMUDMzOCAYBsFzdJzWcuQAQLSkwBFkQgAAhQhAhS4IEmlMAEUzYndHFLAW6AQ8q+IAulnfAXKQTNLygoAgs+jIVrQBsQRheSIgbRhiwoABJ5QMQ/FYsObNMBNYxIxQ5sYwcMsBqhEKbIvyZgIVwhGAcUFhDFKlJRjDy44A/IOAGXEC+AJFgMECRhxjp2LHwoMEAZAQGxPQEgEHK7zRNboIBA2hEkithBArT4g0JSIAGayF4LFpAnGGxhc4cECSVzUABD5uCSlSmCJzsAF0amgIeZPGQpadCAPfIAlU5AwP5iAMZUUlGWVXRALXdgNhz00pap/CVIhBkF47CyksA0ozFB0oA8RQAAOw=="
    },
    "52694552-51c9-4a18-85c5-7aa1f80cd766.gif": {
     "image/gif": "R0lGODlhpABXALMAAP///wAAALq6uiIiIu7u7piYmERERBAQEDIyMtzc3HZ2doiIiMzMzKqqqlRUVGZmZiH5BAEAAAAALAAAAACkAFcAAAT+EMhJq704682nGF0ojmRpnmhFCIURBGksz3StGQdSJK/t/8DghtATGo/IFBGWbDqfliV0Sj1Kq9js7KrtekXcr3isKpLP4zB6nVWz31A3fI6U0+9AO35P0/P/KH6AgyOChIdDZoiLYIqMjxqGkI+Sk4uVloQECi8LBJmWnC+jpAugp6ipqqusra6vsLGys7S1tre4ubq7vL2+v8BjpMPExcbHyMnKy8zNzs/Q0dLT1NXW18fB2tvc3d7fKAoD4wjl4wPlCOef4EIHDgkUL+wABA9M7UACDhUCAQgqAOYD8oCeBE4KLPAb+OOBBQQBBKgwxZBDggUPFGh00ACDAIn+FRwBYACyYgZxJSUsGNCRgz+BJkMQQGAAA4MDCTcgjClinAYGAXJmgJiSZwYHARhsuFc0JD6jGYDCzMADhMd/UDe4KNDBRUsLO7NmeBGPw4IACx9GTPKhlr+nGvwdwCCSqgMHCCgmuGugaT0WLuDCKhAAhNALDRjwCGBQgtQQCQzEI3Bgx0LCFnDoWFyLE8B7yBgsKUvh7GENCOjhkAzgbWMKmEB5BvAgne10A0STtTAgwFcNC37fqznygMMMsTsMeM2GMAiKHhkAZSxBgAEEB17k6Iu6AtFGgpUkfSK9vPnz5g2+DSE3hsHYyRMpbTLdWVoJuzlwui+jAdpCdQX+Mh4eSHHFAUS/yYBUgomEJ+B8dzy2QVU+ZPdegzYQAaEMBLSQDnQ2MLXUWjUAZdUExCEX4Aka0nCWARA+kKINPmXgz2kmKHAASEjdxwCOZTjI4oBKQAQifqTVMNNUFNwEJAlLdESAA+hQwOQF8UVCJAoQHVfBjkGgVMFKDJ5gnAQFEfbJlElikKWKG5pwz1wWAGXERQ5opABHNjCAwF3zLfAnPB286eaWJUx3ZGss2ZLcEtPEqVB+BDBQgDp+wRKfAA106qmnhAX36aeZyvNCOgY80ECbtBiKJaIjLOZlL65GAasI05WZS61lSIrrC77uuqIJLaIA7E+18ArbrSI7IJiBAQbOouwExZ4AFH8SJKBDsqJ40gezIoRKwT6EyiKKMYtCCa4Ie5VznQKsQlWtWELMS28QDjA3UAQAOw=="
    },
    "7994fd02-20c0-4273-a077-3f2d8e08fd10.gif": {
     "image/gif": "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"
    },
    "ec2a8161-1883-49d1-8eae-8b9ff5a42fc3.gif": {
     "image/gif": "R0lGODlhngA1ALMAAP///wAAAMzMzFRUVIiIiJiYmO7u7hAQEHZ2dkRERNzc3CIiImZmZjIyMrq6uqqqqiH5BAEAAAAALAAAAACeADUAAAT+EMhJq704681nOUQnjmRpnmhqNEGRvnAsz5yxHAqt73yfJYEcxjBw+GYEBIFhPDo3ggADM2AMAs0nahCSJB7acIWB1USz4pEjQHEc0mmgwVw+okuMRqUOf8rpdzsIDS4mBwN7CH1afxlnFg0JGQQFAwoGBEkbBA0IcyNSFQuSi06NGI+iCxieAA6drgECHA8NDEIbBqEUC3qlR6cXqR0OTQotAA9TIgIJCYEVussTvb/AAZ+ofLQebCgKeWAZuxMHvtY8wRbDJAPn3wekF+QSAfLoOuoV7CMgKeCENBxKhC8dNkAmjs2SsLCDgAHPOOShoAvaBlxQSumjwE/DLQD+CLwBKIAxQ60BDTesofDgTQkHihzGhLNxwppCvN5JeCALAIEgABR00VCgUzYRA2Y2sPiQUgUF9zg4pXnwAoIBCwIcmgaAhQUDDAqEcBAWJ6sFQ00kWSLu6wEDCFxOSFCSQ4OjjKqaaAuDrw8EkhCYdYCoxIPCYmpyMFtwgz8LKE+8TZM1RdrGjoBakFuCixhdUUco8It5H9YAAyxRKBDaZ6VLmWZKYC0mpMXFpSXqlIBANu85rxStSSmA85EovnPrsHXBcxtjyJQ9FWm1gfXr2K/fZjhQuQ9kkBi3LUA9mua/Bxh7p3EM2gLSE9xpOM/DwAGu63XwxCshIIbHF+j+wp8OBiSwQEr5yUCAcfFdNoFCEyB4jAaDZGfhUhL1lOAMEGHAgG8fheRBSQKsEkYDk20YwwLJzXYPT7P8lINQFtAWxhotqlhCAPAFRR1YYrlSVnM99nCAiSSsxUQMtRyQIwxRDNgfgiIgeWJ5HDgHwBcxHFPkC0VpcNheiIWhmAUr2cTgXnrJ8IAiDDhYwV0lNFCXKW1uMBEF25ywJw0NILKbBQqUKZV6fuQpkKEBPFmloyc8sASVFzggpzCQ9nDmHviNMkEkk2TCQEPt1RjbHZlMOgOlwmik6BD0AFCNBAtY+eB7c3VjAQJtnaMArl7oKMOmE0gjyqBzyjaNfHvRTPCAPLBMgJ+wJxA7QazmbHLQaAxkA+C1EVHw0xzcSkntCNbWg589G9RKyQNHQVjBTwGQ424B8J4bQ7oAdHctpI1iQF4GZPFYT6b6ovuqh+9U5NilHWYj3VyFfJvwC/ymudOac17WhT9KlWlnfx5fXO3CVCiFxqxtvEPAQgeAMZNsQUrwirismqwBvxIo6ZdXFrxZwNAscZENJkMT4JfQROtsAhl3pvCl0/iQdykKiFLdmH22wnC11lXH+s/UYFtDnqFYl52gAzfkrHbZD7itbwQAOw=="
    }
   },
   "cell_type": "markdown",
   "id": "71135395",
   "metadata": {
    "papermill": {
     "duration": 0.006573,
     "end_time": "2024-06-27T17:16:15.132464",
     "exception": false,
     "start_time": "2024-06-27T17:16:15.125891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Model\n",
    "\n",
    "This model has 4 layers \n",
    "1. Input Layer\n",
    "2. Hidden Layer 1\n",
    "3. Hidden Layer 2\n",
    "4. Output Layer \n",
    "\n",
    "### Initialization\n",
    "    here we define all the parameters like input size , hidden layer sizes , learning rates (η) , dropout rate (p) \n",
    "\n",
    "### Sigmoid funtion\n",
    "\n",
    "![CodeCogsEqn.gif](attachment:52694552-51c9-4a18-85c5-7aa1f80cd766.gif)\n",
    "\n",
    "### reLU function\n",
    "\n",
    "![CodeCogsEqn (1).gif](attachment:358609bf-f7b8-4dc6-a6d3-f9e250b38885.gif)\n",
    "\n",
    "### reLU derivative\n",
    "\n",
    "![CodeCogsEqn (2).gif](attachment:ec2a8161-1883-49d1-8eae-8b9ff5a42fc3.gif)\n",
    "\n",
    "\n",
    "### DropOut function \n",
    "\n",
    "![CodeCogsEqn (3).gif](attachment:7994fd02-20c0-4273-a077-3f2d8e08fd10.gif)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454cbb3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T17:16:15.149251Z",
     "iopub.status.busy": "2024-06-27T17:16:15.148835Z",
     "iopub.status.idle": "2024-06-27T17:16:15.174993Z",
     "shell.execute_reply": "2024-06-27T17:16:15.173826Z"
    },
    "papermill": {
     "duration": 0.037608,
     "end_time": "2024-06-27T17:16:15.177585",
     "exception": false,
     "start_time": "2024-06-27T17:16:15.139977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size, eta=0.01, p=0.1, regularization=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.output_size = output_size\n",
    "        self.eta = eta\n",
    "        self.p = p\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        # Weights initialization\n",
    "        self.input_2_hidden_layer_1 = np.random.randn(self.input_size, self.hidden_size_1) * 0.01\n",
    "        self.hidden_layer_1_2_hidden_layer_2 = np.random.randn(self.hidden_size_1, self.hidden_size_2) * 0.01\n",
    "        self.hidden_layer_2_2_output = np.random.randn(self.hidden_size_2, self.output_size) * 0.01\n",
    "        \n",
    "        # Biases initialization\n",
    "        self.b1 = np.zeros((1, self.hidden_size_1))\n",
    "        self.b2 = np.zeros((1, self.hidden_size_2))\n",
    "        self.bo = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def reLU(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def dropout(self, X, p):\n",
    "        prb = 1 - p\n",
    "        M = np.random.rand(*X.shape) < prb\n",
    "        return M * X / prb\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def reLU_derivative(self, X):\n",
    "        return np.where(X > 0, 1, 0)\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, X, isTraining=True):\n",
    "        Z1 = np.dot(X, self.input_2_hidden_layer_1) + self.b1\n",
    "        A1 = self.reLU(Z1)\n",
    "        if isTraining:\n",
    "            A1 = self.dropout(A1, self.p)\n",
    "\n",
    "        Z2 = np.dot(A1, self.hidden_layer_1_2_hidden_layer_2) + self.b2\n",
    "        A2 = self.reLU(Z2)\n",
    "        if isTraining:\n",
    "            A2 = self.dropout(A2, self.p)\n",
    "\n",
    "        Z3 = np.dot(A2, self.hidden_layer_2_2_output) + self.bo\n",
    "        A3 = self.sigmoid(Z3)\n",
    "        \n",
    "        self.Z1 = Z1\n",
    "        self.A1 = A1\n",
    "        self.Z2 = Z2\n",
    "        self.A2 = A2\n",
    "        self.Z3 = Z3\n",
    "        self.A3 = A3\n",
    "\n",
    "        return A3\n",
    "    \n",
    "    def backpropagation(self, X, y, output):\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        self.output_error = output - y.reshape(-1, 1)\n",
    "        self.output_delta = self.output_error / m\n",
    "\n",
    "        self.hidden_layer_2_error = self.output_delta.dot(self.hidden_layer_2_2_output.T)\n",
    "        self.hidden_layer_2_delta = self.hidden_layer_2_error * self.reLU_derivative(self.Z2)\n",
    "\n",
    "        self.hidden_layer_1_error = self.hidden_layer_2_delta.dot(self.hidden_layer_1_2_hidden_layer_2.T)\n",
    "        self.hidden_layer_1_delta = self.hidden_layer_1_error * self.reLU_derivative(self.Z1)\n",
    "\n",
    "        # Update weights with regularization\n",
    "        self.hidden_layer_2_2_output -= (self.A2.T.dot(self.output_delta) + self.regularization * self.hidden_layer_2_2_output) * self.eta\n",
    "        self.hidden_layer_1_2_hidden_layer_2 -= (self.A1.T.dot(self.hidden_layer_2_delta) + self.regularization * self.hidden_layer_1_2_hidden_layer_2) * self.eta\n",
    "        self.input_2_hidden_layer_1 -= (X.T.dot(self.hidden_layer_1_delta) + self.regularization * self.input_2_hidden_layer_1) * self.eta\n",
    "\n",
    "        # Update biases\n",
    "        self.bo -= np.sum(self.output_delta, axis=0, keepdims=True) * self.eta\n",
    "        self.b2 -= np.sum(self.hidden_layer_2_delta, axis=0, keepdims=True) * self.eta\n",
    "        self.b1 -= np.sum(self.hidden_layer_1_delta, axis=0, keepdims=True) * self.eta\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10000, patience=10):\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X_train)\n",
    "            self.backpropagation(X_train, y_train, output)\n",
    "            \n",
    "            # Compute training loss\n",
    "            train_loss = self.cross_entropy_loss(y_train, output)\n",
    "            \n",
    "            # Compute validation loss\n",
    "            val_output = self.forward(X_val, isTraining=False)\n",
    "            val_loss = self.cross_entropy_loss(y_val, val_output)\n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                print(f'Epoch {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                self.save_model('best_model.pkl')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "        \n",
    "        self.load_model('best_model.pkl')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X, isTraining=False)\n",
    "        return np.round(output)\n",
    "    def save_model(self, file_path):\n",
    "        model_data = {\n",
    "            'weights_input_2_hidden_layer_1': self.input_2_hidden_layer_1,\n",
    "            'weights_hidden_layer_1_2_hidden_layer_2': self.hidden_layer_1_2_hidden_layer_2,\n",
    "            'weights_hidden_layer_2_2_output': self.hidden_layer_2_2_output,\n",
    "            'b1': self.b1,\n",
    "            'b2': self.b2,\n",
    "            'bo': self.bo\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "    \n",
    "    def load_model(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "            self.input_2_hidden_layer_1 = model_data['weights_input_2_hidden_layer_1']\n",
    "            self.hidden_layer_1_2_hidden_layer_2 = model_data['weights_hidden_layer_1_2_hidden_layer_2']\n",
    "            self.hidden_layer_2_2_output = model_data['weights_hidden_layer_2_2_output']\n",
    "            self.b1 = model_data['b1']\n",
    "            self.b2 = model_data['b2']\n",
    "            self.bo = model_data['bo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf1027c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T17:16:15.194571Z",
     "iopub.status.busy": "2024-06-27T17:16:15.193621Z",
     "iopub.status.idle": "2024-06-27T17:16:15.273157Z",
     "shell.execute_reply": "2024-06-27T17:16:15.272091Z"
    },
    "papermill": {
     "duration": 0.091211,
     "end_time": "2024-06-27T17:16:15.275848",
     "exception": false,
     "start_time": "2024-06-27T17:16:15.184637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(all_res)\n",
    "\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(X,gens_,test_size=0.45)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_temp,y_temp,test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab5b607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T17:16:15.292540Z",
     "iopub.status.busy": "2024-06-27T17:16:15.291703Z",
     "iopub.status.idle": "2024-06-27T18:03:35.776716Z",
     "shell.execute_reply": "2024-06-27T18:03:35.775229Z"
    },
    "papermill": {
     "duration": 2840.499345,
     "end_time": "2024-06-27T18:03:35.782650",
     "exception": false,
     "start_time": "2024-06-27T17:16:15.283305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 0.6931518714552818, Validation Loss: 0.69235138576567\n",
      "Epoch 1000, Training Loss: 0.5237549819063702, Validation Loss: 0.5277308357889177\n",
      "Epoch 2000, Training Loss: 0.5200862724506716, Validation Loss: 0.5246380247619051\n",
      "Epoch 3000, Training Loss: 0.5153793954545796, Validation Loss: 0.5200638434187566\n",
      "Epoch 4000, Training Loss: 0.35091275615234124, Validation Loss: 0.35309894238790335\n",
      "Epoch 5000, Training Loss: 0.19503708931336783, Validation Loss: 0.2034215401214823\n",
      "Epoch 6000, Training Loss: 0.16806931099402225, Validation Loss: 0.1799714703885128\n",
      "Epoch 7000, Training Loss: 0.1593203886889041, Validation Loss: 0.1719559683228419\n",
      "Epoch 8000, Training Loss: 0.1537586363925055, Validation Loss: 0.16678527735903897\n",
      "Epoch 9000, Training Loss: 0.15022566223059275, Validation Loss: 0.16351601747624328\n",
      "Epoch 10000, Training Loss: 0.1463143511815475, Validation Loss: 0.1612906906533961\n",
      "Epoch 11000, Training Loss: 0.1448959446285285, Validation Loss: 0.1594882872207232\n",
      "Epoch 12000, Training Loss: 0.14248443325125842, Validation Loss: 0.1579852152043598\n",
      "Epoch 13000, Training Loss: 0.1415956322175863, Validation Loss: 0.1567006575212537\n",
      "Epoch 14000, Training Loss: 0.13936249944145968, Validation Loss: 0.1555346972796903\n",
      "Epoch 15000, Training Loss: 0.13818940634676483, Validation Loss: 0.15450488126102468\n",
      "Epoch 16000, Training Loss: 0.13587290259214363, Validation Loss: 0.1535761633169728\n",
      "Epoch 17000, Training Loss: 0.13431384441597322, Validation Loss: 0.15271805074439432\n",
      "Epoch 18000, Training Loss: 0.13447259165568534, Validation Loss: 0.15194283771242514\n",
      "Epoch 19000, Training Loss: 0.13300307563703345, Validation Loss: 0.1512377765818213\n",
      "Epoch 20000, Training Loss: 0.13456864100694887, Validation Loss: 0.1505755352805413\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(all_res.shape[1],64,64,1)\n",
    "model.train(X_train,y_train,X_val,y_val,epochs=21000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d67a42bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T18:03:35.819576Z",
     "iopub.status.busy": "2024-06-27T18:03:35.818996Z",
     "iopub.status.idle": "2024-06-27T18:03:35.839034Z",
     "shell.execute_reply": "2024-06-27T18:03:35.837737Z"
    },
    "papermill": {
     "duration": 0.044708,
     "end_time": "2024-06-27T18:03:35.843156",
     "exception": false,
     "start_time": "2024-06-27T18:03:35.798448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e1494",
   "metadata": {
    "papermill": {
     "duration": 0.014452,
     "end_time": "2024-06-27T18:03:35.872513",
     "exception": false,
     "start_time": "2024-06-27T18:03:35.858061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Here we are getting close to 95% accuracy in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b7810f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T18:03:35.908381Z",
     "iopub.status.busy": "2024-06-27T18:03:35.907811Z",
     "iopub.status.idle": "2024-06-27T18:03:35.923132Z",
     "shell.execute_reply": "2024-06-27T18:03:35.921733Z"
    },
    "papermill": {
     "duration": 0.036508,
     "end_time": "2024-06-27T18:03:35.926902",
     "exception": false,
     "start_time": "2024-06-27T18:03:35.890394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9492583918813428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,accuracy_score\n",
    "\n",
    "# r2 = r2_score(y_test,y_pred)\n",
    "# print(r2)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5793,
     "sourceId": 9812,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5281415,
     "sourceId": 8785281,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4103.366787,
   "end_time": "2024-06-27T18:03:36.680993",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-27T16:55:13.314206",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
